{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tf_utils import load_features, random_mini_batches, convert_to_one_hot, predict\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 128, 28, 1])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    #input shape [batch, 128, 28, 1]\n",
    "    #output shape [batch, 128, 28, 80]    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=80,\n",
    "      kernel_size=[57, 6],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # Input Tensor Shape: [batch_size, 128, 28, 80]\n",
    "    # Output Tensor Shape: [batch_size, 64, 14, 80]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #2\n",
    "    #input shape [batch_size, 64, 14, 80]\n",
    "    #output shape [batch_size, 64, 14, 80]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=pool1,\n",
    "      filters=80,\n",
    "      kernel_size=[1, 3],\n",
    "      padding=\"same\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Input Tensor Shape: [batch_size, 64, 14, 80]\n",
    "    # Output Tensor Shape: [batch_size, 8, 2, 80]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[8, 7], strides=[8, 7])\n",
    "\n",
    "    # Flatten tensor\n",
    "    # Input Tensor Shape: [batch_size, 8, 2, 80]\n",
    "    # Output Tensor Shape: [batch_size, 8 * 2 * 80]\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 8 * 2 * 80])\n",
    "\n",
    "    # Dense Layer\n",
    "    dense1 = tf.layers.dense(inputs=pool2_flat, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "#     dropout1 = tf.layers.dropout(inputs=dense1, rate=0.5, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    # Dense Layer\n",
    "    dense2 = tf.layers.dense(inputs=dense1, units=1024, activation=tf.nn.relu)\n",
    "\n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "#     dropout2 = tf.layers.dropout(inputs=dense2, rate=0.5, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 10]\n",
    "    logits = tf.layers.dense(inputs=dense2, units=2)\n",
    "\n",
    "    predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"sparse_softmax_cross_entropy_loss/value:0\", shape=(), dtype=float32)\n",
      "Tensor(\"sparse_softmax_cross_entropy_loss/value:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEWCAYAAABBvWFzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqQm/j8TR1VW5z3jqgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122435dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.68472904, 'loss': 0.67006814, 'global_step': 300}\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "losses = []\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load training and eval data\n",
    "train_data, eval_data, train_labels, eval_labels = load_features()\n",
    "train_data = train_data.astype(np.float32)\n",
    "eval_data = eval_data.astype(np.float32)    \n",
    "\n",
    "# Create the Estimator\n",
    "sound_classifier = tf.estimator.Estimator(\n",
    "  model_fn=cnn_model_fn, model_dir=\"tmp/audio_convnet_model\")\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": train_data}, y=train_labels, batch_size=1, num_epochs=None, shuffle=True\n",
    ")\n",
    "sound_classifier.train(input_fn=train_input_fn, steps=200)\n",
    "\n",
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x={\"x\": eval_data},\n",
    "  y=eval_labels,\n",
    "  num_epochs=1,\n",
    "  shuffle=False)\n",
    "eval_results = sound_classifier.evaluate(input_fn=eval_input_fn)\n",
    "\n",
    "plt.plot(np.squeeze(losses))\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "    \n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 0 in both shapes must be equal, but are 10 and 1. Shapes are [10,2] and [1,10]. for 'softmax_cross_entropy_with_logits_sg' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [10,2], [1,10].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    685\u001b[0m           \u001b[0mgraph_def_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m           input_tensors_as_shapes, status)\n\u001b[0m\u001b[1;32m    687\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 0 in both shapes must be equal, but are 10 and 1. Shapes are [10,2] and [1,10]. for 'softmax_cross_entropy_with_logits_sg' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [10,2], [1,10].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-01adccf6d86b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m   \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-55-01adccf6d86b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(unused_argv)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     )\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0msound_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_input_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Evaluate the model and print results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    741\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 743\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    744\u001b[0m       \u001b[0;31m# Check if the user created a loss summary, and add one if they didn't.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0;31m# We assume here that the summary is called 'loss'. If it is not, we will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_fn_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'config'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_fn_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_fn_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEstimatorSpec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-80d8189e9e49>\u001b[0m in \u001b[0;36mcnn_model_fn\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;31m# Calculate Loss (for both TRAIN and EVAL modes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m#     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m               \u001b[0;34m'in a future version'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'after %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m               instructions)\n\u001b[0;32m--> 136\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m     return tf_decorator.make_decorator(\n\u001b[1;32m    138\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprecated'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1884\u001b[0m   return softmax_cross_entropy_with_logits_v2(\n\u001b[0;32m-> 1885\u001b[0;31m       labels=labels, logits=logits, dim=dim, name=name)\n\u001b[0m\u001b[1;32m   1886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msoftmax_cross_entropy_with_logits_v2\u001b[0;34m(_sentinel, labels, logits, dim, name)\u001b[0m\n\u001b[1;32m   1802\u001b[0m     \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m     cost, unused_backprop = gen_nn_ops._softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 1804\u001b[0;31m         precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   1805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[0;31m# The output cost shape should be the input minus dim.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   4622\u001b[0m     _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[1;32m   4623\u001b[0m         \u001b[0;34m\"SoftmaxCrossEntropyWithLogits\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4624\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m   4625\u001b[0m     \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4626\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    788\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3160\u001b[0m         op_def=op_def)\n\u001b[1;32m   3161\u001b[0m     self._create_op_helper(ret, compute_shapes=compute_shapes,\n\u001b[0;32m-> 3162\u001b[0;31m                            compute_device=compute_device)\n\u001b[0m\u001b[1;32m   3163\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_create_op_helper\u001b[0;34m(self, op, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   3206\u001b[0m     \u001b[0;31m# compute_shapes argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c_op\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3208\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3209\u001b[0m     \u001b[0;31m# TODO(b/XXXX): move to Operation.__init__ once _USE_C_API flag is removed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3210\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2425\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs_c_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2426\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2427\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_set_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_set_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2398\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2400\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2401\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2402\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2329\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2330\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2332\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, require_shape_fn)\u001b[0m\n\u001b[1;32m    625\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    626\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m                                   require_shape_fn)\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, require_shape_fn)\u001b[0m\n\u001b[1;32m    689\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 0 in both shapes must be equal, but are 10 and 1. Shapes are [10,2] and [1,10]. for 'softmax_cross_entropy_with_logits_sg' (op: 'SoftmaxCrossEntropyWithLogits') with input shapes: [10,2], [1,10]."
     ]
    }
   ],
   "source": [
    "def main(unused_argv):\n",
    "    # Load training and eval data\n",
    "    train_data, eval_data, train_labels, eval_labels = load_features()\n",
    "    train_data = train_data.astype(np.float32)\n",
    "    eval_data = eval_data.astype(np.float32)    \n",
    "    \n",
    "    # Create the Estimator\n",
    "    sound_classifier = tf.estimator.Estimator(\n",
    "      model_fn=cnn_model_fn, model_dir=\"tmp/audio_convnet_model\")\n",
    "\n",
    "    # Train the model\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_data}, y=train_labels, batch_size=10, num_epochs=None, shuffle=True\n",
    "    )\n",
    "    sound_classifier.train(input_fn=train_input_fn, steps=100)\n",
    "    \n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "    eval_results = sound_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.reset_default_graph()\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'sparse_softmax_cross_entropy_loss/value:0' shape=() dtype=float32>,\n",
       " <tf.Tensor 'sparse_softmax_cross_entropy_loss/value:0' shape=() dtype=float32>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
