{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(dataset='training'):    \n",
    "    return pd.read_pickle('../data_processed/' + dataset + '_set.pkl')\n",
    "\n",
    "def get_dimensions(mel_shape, shape='stacked'):\n",
    "    if shape =='flat':   \n",
    "        mel_depth = 1\n",
    "        mel_height = 256\n",
    "    elif shape == 'stacked':\n",
    "        mel_depth = 2\n",
    "        mel_height = 128        \n",
    "    \n",
    "    mel_width = int(mel_shape[1])    \n",
    "    return mel_height, mel_width, mel_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(dataset='training', features=['Mel'], shape='mel_only', window_size=28):\n",
    "    \n",
    "    df = load_data(dataset=dataset)\n",
    "        \n",
    "    #Where it will be stored\n",
    "    files, labels, data = [],[],[]\n",
    "    \n",
    "    #List of file names in the dataset\n",
    "    file_names = list(df.File_id.unique())\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        #Load the needed columns, and stack them, move the volume dim to the end\n",
    "        mel = np.array(row[features])\n",
    "        mel = np.stack((mel)) \n",
    "        \n",
    "        #obtain some dimentions about the set to load\n",
    "        if len(features) > 1:\n",
    "            mel_height, mel_width, mel_depth = get_dimensions(shape=shape, mel_shape=mel.shape)\n",
    "        else:\n",
    "            mel_height, mel_width, mel_depth = mel.shape[1], mel.shape[2], mel.shape[0]\n",
    "        \n",
    "        #each mel needs to be chopped into segments of window_size width\n",
    "        batch_size = int(mel.shape[2] / window_size)\n",
    "        \n",
    "        #reshape mel and remove parts that will be ignored\n",
    "        mel = np.reshape(mel[:,:,0:batch_size*window_size], (mel_depth, mel_height, batch_size*window_size))                \n",
    "        \n",
    "        for i in list(range(batch_size)):\n",
    "            labels.append(row['Label'])\n",
    "            files.append(row['File_id'])           \n",
    "            data.append(mel[:,:,i*window_size:(i+1)*window_size])                        \n",
    "            \n",
    "    return np.array(data, dtype=np.float32), np.array(labels), np.array(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train, f_train = process_files(dataset='training', features=['Mel', 'Mel_deltas'], \n",
    "                                                    shape='stacked', window_size=28)\n",
    "x_test, y_test, f_test = process_files(dataset='test', features=['Mel', 'Mel_deltas'], \n",
    "                                                    shape='stacked', window_size=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(577, 2, 128, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
