{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import (\n",
    "#   read_data, \n",
    "#   input_setup, \n",
    "#   imsave,\n",
    "#   merge\n",
    "# )\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class soundCNN(object):\n",
    "\n",
    "    def __init__(self, \n",
    "               sess, \n",
    "               file_length=128,\n",
    "               file_height=400\n",
    "               number_of_classes=2, \n",
    "               batch_size=1,\n",
    "               checkpoint_dir=None, \n",
    "               sample_dir=None):\n",
    "\n",
    "    self.sess = sess\n",
    "    self.file_length = file_length\n",
    "    self.file_height = file_height\n",
    "    self.batch_size = batch_size\n",
    "    self.number_of_classes = number_of_classes\n",
    "\n",
    "    self.checkpoint_dir = checkpoint_dir\n",
    "    self.sample_dir = sample_dir\n",
    "    self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "    self.sounds = tf.placeholder(tf.float32, [self.file_length, self.file_height, 1, None], name='sounds')\n",
    "    self.labels = tf.placeholder(tf.float32, [self.number_of_classes, None], name='labels')\n",
    "\n",
    "    self.weights = {\n",
    "      'w1': tf.Variable(tf.random_normal([57, 6, 1, 80], stddev=1e-3), name='w1'),\n",
    "      'w2': tf.Variable(tf.random_normal([1, 3, 1, 80], stddev=1e-3), name='w2'),\n",
    "      'w3': tf.Variable(tf.random_normal([50, 1], stddev=1e-3), name='w3'),\n",
    "      'w4': tf.Variable(tf.random_normal([50, 1], stddev=1e-3), name='w4')        \n",
    "    }\n",
    "    self.biases = {\n",
    "      'b1': tf.Variable(tf.zeros([80]), name='b1'),\n",
    "      'b2': tf.Variable(tf.zeros([80]), name='b2'),\n",
    "      'b3': tf.Variable(tf.zeros([50]), name='b3'),\n",
    "      'b4': tf.Variable(tf.zeros([50]), name='b4')        \n",
    "    }\n",
    "\n",
    "    self.pred = self.model()\n",
    "\n",
    "    # Loss function Revise this as unclear what we should be using.\n",
    "#     self.loss = tf.reduce_mean(tf.square(self.labels - self.pred))\n",
    "\n",
    "#     self.saver = tf.train.Saver()\n",
    "\n",
    "    def compute_cost(Z3, Y):\n",
    "        logits = tf.transpose(Z3)\n",
    "        labels = tf.transpose(Y)\n",
    "        \n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "        return cost   \n",
    "\n",
    "    def train(self, config):\n",
    "    if config.is_train:\n",
    "      input_setup(self.sess, config)\n",
    "    else:\n",
    "      nx, ny = input_setup(self.sess, config)\n",
    "\n",
    "    if config.is_train:     \n",
    "      data_dir = os.path.join('./{}'.format(config.checkpoint_dir), \"train.h5\")\n",
    "    else:\n",
    "      data_dir = os.path.join('./{}'.format(config.checkpoint_dir), \"test.h5\")\n",
    "\n",
    "    train_data, train_label = read_data(data_dir)\n",
    "\n",
    "    # Stochastic gradient descent with the standard backpropagation\n",
    "    self.train_op = tf.train.GradientDescentOptimizer(config.learning_rate).minimize(self.loss)\n",
    "\n",
    "    tf.initialize_all_variables().run()\n",
    "\n",
    "    counter = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    if self.load(self.checkpoint_dir):\n",
    "      print(\" [*] Load SUCCESS\")\n",
    "    else:\n",
    "      print(\" [!] Load failed...\")\n",
    "\n",
    "    if config.is_train:\n",
    "      print(\"Training...\")\n",
    "\n",
    "      for ep in xrange(config.epoch):\n",
    "        # Run by batch images\n",
    "        batch_idxs = len(train_data) // config.batch_size\n",
    "        for idx in xrange(0, batch_idxs):\n",
    "          batch_images = train_data[idx*config.batch_size : (idx+1)*config.batch_size]\n",
    "          batch_labels = train_label[idx*config.batch_size : (idx+1)*config.batch_size]\n",
    "\n",
    "          counter += 1\n",
    "          _, err = self.sess.run([self.train_op, self.loss], feed_dict={self.images: batch_images, self.labels: batch_labels})\n",
    "\n",
    "          if counter % 10 == 0:\n",
    "            print(\"Epoch: [%2d], step: [%2d], time: [%4.4f], loss: [%.8f]\" \\\n",
    "              % ((ep+1), counter, time.time()-start_time, err))\n",
    "\n",
    "          if counter % 500 == 0:\n",
    "            self.save(config.checkpoint_dir, counter)\n",
    "\n",
    "    else:\n",
    "      print(\"Testing...\")\n",
    "\n",
    "      result = self.pred.eval({self.images: train_data, self.labels: train_label})\n",
    "\n",
    "      result = merge(result, [nx, ny])\n",
    "      result = result.squeeze()\n",
    "      image_path = os.path.join(os.getcwd(), config.sample_dir)\n",
    "      image_path = os.path.join(image_path, \"test_image.png\")\n",
    "      imsave(result, image_path)\n",
    "\n",
    "  def model(self):\n",
    "    conv1 = tf.nn.relu(tf.nn.conv2d(self.images, self.weights['w1'], strides=[1,1,1,1], padding='VALID') + self.biases['b1'])\n",
    "    conv2 = tf.nn.relu(tf.nn.conv2d(conv1, self.weights['w2'], strides=[1,1,1,1], padding='VALID') + self.biases['b2'])\n",
    "    conv3 = tf.nn.conv2d(conv2, self.weights['w3'], strides=[1,1,1,1], padding='VALID') + self.biases['b3']\n",
    "    return conv3\n",
    "\n",
    "  def save(self, checkpoint_dir, step):\n",
    "    model_name = \"SRCNN.model\"\n",
    "    model_dir = \"%s_%s\" % (\"srcnn\", self.label_size)\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "\n",
    "    self.saver.save(self.sess,\n",
    "                    os.path.join(checkpoint_dir, model_name),\n",
    "                    global_step=step)\n",
    "\n",
    "  def load(self, checkpoint_dir):\n",
    "    print(\" [*] Reading checkpoints...\")\n",
    "    model_dir = \"%s_%s\" % (\"srcnn\", self.label_size)\n",
    "    checkpoint_dir = os.path.join(checkpoint_dir, model_dir)\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
